{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadrature formulas\n",
    "\n",
    "We're trying to find quadrature formulas on arbitrary triangles that satisfy\n",
    "\n",
    "$$|T|\\sum_i w_if(x_i) =  \\int_T f(x)$$\n",
    "\n",
    "and are exact for a certain polynomial degree $P_k$, $k>0$. Let $\\{p_i\\}_{1\\le i\\le n}$ be a basis for $P_k$. These can either be monomials of the form $x^iy^j$, but, for stability reasons, we instead choose orthogonal polynomials. The (nonlinear) system of equations we have to solve is given by\n",
    "\n",
    "$$\n",
    "A(x)w:=|T| \\begin{pmatrix} \n",
    "p_1(x_1) & \\ldots  & p_1(x_m) \\\\\n",
    "\\vdots &  &  \\vdots \\\\\n",
    "\\vdots &  &  \\vdots \\\\\n",
    "p_n(x_1) & \\ldots & p_n(x_m)\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} w_1 \\\\ \\vdots \\\\ w_m \\end{pmatrix}\n",
    "=  \\begin{pmatrix} \\int_T p_1 \\\\ \\vdots\\\\ \\vdots \\\\ \\int_T p_n \\end{pmatrix}=: b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first think about how big these systems are. The number of rows is easy to determine, as corresponds to the desired polynomial degree $k$. In 2d, there are $n = 1/2(k+1)(k+2)$ basis functions spanning $P_k$. As for the number of columns... well this is more difficult. One can theoretically add as many points as one wants, but many of them may be redundant, meaning we could achieve the same degree of integration with fewer points. In general however, the optimal number of points required is less than the \"size\" of the polynomial degree. This means that $A(x)$ is a \"tall\" matrix, making the system of equations underdetermined.\n",
    "\n",
    "However, the rank of $A(x)$ is still full, almost always. (why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System to solve\n",
    "\n",
    "Let us redefine the problem into a root-finding problem of the form\n",
    "\n",
    "$$F(x,w) = A(x)w-b \\stackrel{!}{=}0$$\n",
    "\n",
    "Since the system is underdetermined, we can calculate \n",
    "\n",
    "$$w = w(x) = [A(x)^\\top A(x)]^{-1}A(x)^\\top b$$\n",
    "\n",
    "and redefine the problem as\n",
    "\n",
    "$$ F(x) = A(x)w(x)-b= A(x)[A(x)^\\top A(x)]^{-1}A(x)^\\top b-b \\stackrel{!}{=} 0$$\n",
    "\n",
    "depending solely on the parameter $x$. This would mean that the normal solution is exact and satisfies the original system. Note that $F:R^m\\to R^n$, $m<n$. One way to solve this underdetermined system is by using GauÃŸ-Newton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobians\n",
    "\n",
    "For this, we first need to determine the Jacobian of $F$, which we henceforth call $J_F$. For this,let us first define \n",
    "\n",
    "$$B(x) = A(x)^\\top A(x),\\qquad C(x) = B(x)^{-1}$$\n",
    "\n",
    "Thus, differentiationg with respect to the $i$-th component of $x$, we get\n",
    "\n",
    "$$\\begin{align}\n",
    "A_i'(x) &:= \\partial_{x_i} A(x) \\\\\n",
    "B_i'(x) &:= \\partial_{x_i} B(x) = A_i'(x)^\\top A(x) +  A(x)^\\top A_i'(x) \\\\\n",
    "C_i'(x) &:= \\partial_{x_i} C(x) = -C(x)B_i'(x)C(x) \\\\\n",
    "F_i'(x) &:= \\partial_{x_i} F(x) = \\partial_{x_i}[A(x)C(x)A(x)^\\top b] \\\\\n",
    "& = A_i'(x)C(x)A(x)^\\top b -A(x)C_i'(x)A(x)^\\top b + A(x)C(x)A_i'(x)^\\top b\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note: If one takes orthogonal polynomials, $b$ is zero up to the first entry. Also, the first polynomial is constant, and thus the first row in $A(x)$ does not depend on $x$. As such, $A_i'(x)^\\top b=0$, and the third term vanishes, which yields\n",
    "\n",
    "$$\n",
    "F_i'(x) = A_i'(x)C(x)A(x)^\\top b -A(x)C_i'(x)A(x)^\\top b\n",
    "$$\n",
    "\n",
    "and $J_F(x) = \\Big(F_1'(x) \\;\\ldots \\; F_m'(x)\\Big)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss-Newton\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Gauss-Newton, we redefine the problem as a minimization problem of the form\n",
    "\n",
    "$$\\min_x \\frac12 \\| F(x)\\|_2^2 $$\n",
    "\n",
    "The first optimality condition is given by\n",
    "\n",
    "$$G(x):=J_F(x)^\\top F(x)\\stackrel{!}{=} 0$$\n",
    "\n",
    "with derivative (Jacobian)\n",
    "\n",
    "$$ J_G(x) = J_F(x)^\\top J_F(x) + \"H_F(x) F(x)\" \\approx J_F(x)^\\top J_F(x)$$\n",
    "\n",
    "The higher order terms from the Hessian are ignored... making the whole thing an \"inexact/quasi\" Newton method. The Newton iteration reads\n",
    "\n",
    "$$ x_{n+1} = x_n - J_G(x_n)^{-1}G(x_n) = x_n - [J_F(x_n)^\\top J_F(x_n)]^{-1}J_F(x_n)^\\top F(x_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief discussion\n",
    "\n",
    "This methods works fine... most of the time. However, it may happen that the points lie outside the triangle when the iteration converges... even if the initial guess only has points in the interior. Further, one might be only interested in quadrature rules that only use positive weights. The method presented above provides no guarantees that this will happen... To impose certain constarints, one might look into barrier methods.\n",
    "\n",
    "Let's say we want to impose $w(x)>0$ in each component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
