{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadrature formulas\n",
    "\n",
    "We're trying to find quadrature formulas on arbitrary triangles that satisfy\n",
    "\n",
    "$$|T|\\sum_i w_if(x_i) =  \\int_T f(x)$$\n",
    "\n",
    "and are exact for a certain polynomial degree $P_k$, $k>0$. Let $\\{p_i\\}_{1\\le i\\le n}$ be a basis for $P_k$. These can either be monomials of the form $x^iy^j$, but, for stability reasons, we instead choose orthogonal polynomials. The (nonlinear) system of equations we have to solve is given by\n",
    "\n",
    "$$\n",
    "A(x)w:=|T| \\begin{pmatrix} \n",
    "p_1(x_1) & \\ldots  & p_1(x_m) \\\\\n",
    "\\vdots &  &  \\vdots \\\\\n",
    "\\vdots &  &  \\vdots \\\\\n",
    "p_n(x_1) & \\ldots & p_n(x_m)\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} w_1 \\\\ \\vdots \\\\ w_m \\end{pmatrix}\n",
    "=  \\begin{pmatrix} \\int_T p_1 \\\\ \\vdots\\\\ \\vdots \\\\ \\int_T p_n \\end{pmatrix}=: b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first think about how big these systems are. The number of rows is easy to determine, as corresponds to the desired polynomial degree $k$. In 2d, there are $n = 1/2(k+1)(k+2)$ basis functions spanning $P_k$. As for the number of columns... well this is more difficult. One can theoretically add as many points as one wants, but many of them may be redundant, meaning we could achieve the same degree of integration with fewer points. In general however, the optimal number of points required is less than the \"size\" of the polynomial degree. This means that $A(x)$ is a \"tall\" matrix, making the system of equations underdetermined.\n",
    "\n",
    "However, the rank of $A(x)$ is still full, almost always. (why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System to solve\n",
    "\n",
    "Let us redefine the problem into a root-finding problem of the form\n",
    "\n",
    "$$F(x,w) = A(x)w-b \\stackrel{!}{=}0$$\n",
    "\n",
    "Since the system is underdetermined, we can calculate \n",
    "\n",
    "$$w = w(x) = [A(x)^\\top A(x)]^{-1}A(x)^\\top b$$\n",
    "\n",
    "and redefine the problem as\n",
    "\n",
    "$$ F(x) = A(x)w(x)-b= A(x)[A(x)^\\top A(x)]^{-1}A(x)^\\top b-b \\stackrel{!}{=} 0$$\n",
    "\n",
    "depending solely on the parameter $x$. This would mean that the normal solution is exact and satisfies the original system. Note that $F:R^m\\to R^n$, $m<n$. One way to solve this underdetermined system is by using GauÃŸ-Newton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobians\n",
    "\n",
    "For this, we first need to determine the Jacobian of $F$, which we henceforth call $J_F$. For this,let us first define \n",
    "\n",
    "$$B(x) = A(x)^\\top A(x),\\qquad C(x) = B(x)^{-1}$$\n",
    "\n",
    "Thus, differentiationg with respect to the $i$-th component of $x$, we get\n",
    "\n",
    "$$\\begin{align}\n",
    "A_i'(x) &:= \\partial_{x_i} A(x) \\\\\n",
    "B_i'(x) &:= \\partial_{x_i} B(x) = A_i'(x)^\\top A(x) +  A(x)^\\top A_i'(x) \\\\\n",
    "C_i'(x) &:= \\partial_{x_i} C(x) = -C(x)B_i'(x)C(x) \\\\\n",
    "F_i'(x) &:= \\partial_{x_i} F(x) = \\partial_{x_i}[A(x)C(x)A(x)^\\top b] \\\\\n",
    "& = A_i'(x)C(x)A(x)^\\top b -A(x)C_i'(x)A(x)^\\top b + A(x)C(x)A_i'(x)^\\top b\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note: If one takes orthogonal polynomials, $b$ is zero up to the first entry. Also, the first polynomial is constant, and thus the first row in $A(x)$ does not depend on $x$. As such, $A_i'(x)^\\top b=0$, and the third term vanishes, which yields\n",
    "\n",
    "$$\n",
    "F_i'(x) = A_i'(x)C(x)A(x)^\\top b -A(x)C_i'(x)A(x)^\\top b\n",
    "$$\n",
    "\n",
    "and $J_F(x) = \\Big(F_1'(x) \\;\\ldots \\; F_m'(x)\\Big)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss-Newton\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Gauss-Newton, we redefine the problem as a minimization problem of the form\n",
    "\n",
    "$$\\min_x \\tfrac12 \\| F(x)\\|_2^2 $$\n",
    "\n",
    "The first optimality condition is given by\n",
    "\n",
    "$$G(x):=J_F(x)^\\top F(x)\\stackrel{!}{=} 0$$\n",
    "\n",
    "with derivative (Jacobian)\n",
    "\n",
    "$$ J_G(x) = J_F(x)^\\top J_F(x) + \"H_F(x) F(x)\" \\approx J_F(x)^\\top J_F(x)$$\n",
    "\n",
    "The higher order terms from the Hessian are ignored... making the whole thing an \"inexact/quasi\" Newton method. The Newton iteration reads\n",
    "\n",
    "$$ x_{n+1} = x_n - J_G(x_n)^{-1}G(x_n) = x_n - [J_F(x_n)^\\top J_F(x_n)]^{-1}J_F(x_n)^\\top F(x_n)$$\n",
    "\n",
    "This could also be derived as an exact Neton step for $F(x)=0$, where the inverse of the Jacobian of $F$ is computed using the pseudo-inverse. However, deriving it as the solution to an optimization problem has certain advantages, which we will touch upon in a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barrier functions\n",
    "\n",
    "This methods works fine... most of the time. However, it may happen that the points lie outside the triangle when the iteration converges... even if the initial guess only has points in the interior. Further, one might be only interested in quadrature rules that only use positive weights. The method presented above provides no guarantees that this will happen... To impose certain constarints, one might look into **barrier functions**, like a logarithmic one.\n",
    "\n",
    "Let's say we want to impose $w(x)>0$, i.e. all weights are positive. Then the problem to solve turns into a constrained optimization problem, which reads\n",
    "\n",
    "$$\\min_x \\tfrac12 \\| F(x)\\|_2^2 \\qquad \\text{subject to} \\qquad w(x)>0$$\n",
    "\n",
    "An equivalent problem is to instead solve a sequence of unconstrained optimization problems\n",
    "\n",
    "$$\\min_x \\tfrac12 \\| F(x)\\|_2^2 -\\tfrac1L\\log(w(x)), \\;\\; L>0$$\n",
    "\n",
    "by increasing the value of $L$ each time and starting the Gauss-Newton iteration with the last result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here being that if you start with a positive $w(x)$ and a moderate $L$, the minimizer is discouraged of driving the values close to zero and into the negative, where the logarithm is undefined. As the value of $L$ increases, the weights can get closer to zero, but still not negative. In the limit case, it is expected that we reach a solution for the original problem, but with positive weights. If $L$ is large enough, we can drop the logarithm term altogether, and solve the original problem, since the starting value is most likely close enough to not diverge to a solution with negative weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
